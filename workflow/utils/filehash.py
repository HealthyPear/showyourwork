from . import paths
from . import exceptions
from .config import get_snakemake_variable
from pathlib import Path
import hashlib
import snakemake
import json


__all__ = ["get_deps_hash"]


class get_deps_hash:
    def __init__(self, file):
        # Note that `file` should be relative to `paths.user`
        self.file = Path(file).as_posix()

    def _get_direct_dependencies(self, file):
        # Get the inputs to all jobs that produce this file
        # Note that there should only be at most 1 job if
        # the workflow has no ambiguous rules (and even if it
        # does, the DAG should choose a single job, I think)
        inputs = [job.input for job in self.dag.jobs if file in job.rule.output]

        # Return a set
        return set([Path(item).as_posix() for sublist in inputs for item in sublist])

    def _get_all_dependencies(self, file):
        # Recursively get all dependencies
        deps = self._get_direct_dependencies(file)
        new_deps = set()
        for dep in deps:
            new_deps.update(self._get_all_dependencies(dep))
        deps.update(new_deps)
        return deps

    def _get_file_hash(self, file, buf_sz=2 ** 16):
        """
        Return a custom hash for a dependency.

        Dependencies can either just **exist** (as files comitted to the repo)
        or they can be programmatically generated by a job. In the former case,
        we compute the file's SHA256 hash. In the latter case, we record the
        parameters of the generating rule (since the file may not exist yet).

        """
        for job in self.dag.jobs:

            # Return the hash of the rule generating the file
            # Assumes there's only 1 job in the DAG that can
            # generate this file (see above)

            if file in job.rule.output:

                # Get all inputs, including script & conda env
                inputs = set(job.unique_input)
                if job.rule.conda_env:
                    inputs.add(job.rule.conda_env)
                if job.rule.script:
                    inputs.add(job.rule.script)
                inputs = sorted(inputs)

                # Assemble all relevant metadata
                # TODO: Are there other job settings we should track here?
                meta = {
                    "input": inputs,
                    "params": dict(job.params),
                    "conda": job.rule.conda_env,
                    "shell": job.shellcmd,
                    "script": job.rule.script,
                }
                return {file: meta}

        else:

            # Return the hash of the file itself

            if not (paths.user / file).exists():
                # TODO (this shouldn't happen)
                raise exceptions.ShowyourworkException()

            # TODO: Progress bar?
            sha256 = hashlib.sha256()
            with open(paths.user / file, "rb") as f:
                while True:
                    data = f.read(buf_sz)
                    if not data:
                        break
                    sha256.update(data)
            return {file: {"sha256": sha256.hexdigest()}}

    def __call__(self, wildcards):
        # Wait until the DAG has been built
        snakemake.workflow.checkpoints.blocking.get()

        # Get the workflow graph
        self.dag = get_snakemake_variable("dag", None)
        if self.dag is None:
            # TODO
            raise exceptions.ShowyourworkException()

        # TODO: Handle workflows with user-defined
        # checkpoints. Currently there's no way to
        # infer the true dependencies of a file if
        # they originate from a checkpoint without
        # running the workflow twice!
        if len(list(self.dag.checkpoint_jobs)):
            # TODO
            raise exceptions.ShowyourworkException()

        # Get all dependencies of the file
        deps = self._get_all_dependencies(self.file)

        # Compute a custom hash for each dependency
        meta = {}
        for dep in sorted(deps):
            meta.update(self._get_file_hash(dep))

        # Save to a temp file on disk
        hash_file = paths.deps / f"{self.file}.json"
        hash_file.parents[0].mkdir(exist_ok=True, parents=True)
        with open(hash_file, "w") as f:
            json.dump(meta, f, indent=4)

        return hash_file.relative_to(paths.user).as_posix()